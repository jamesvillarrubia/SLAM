2018-02-26 10:20:07,489 [INFO] IRT.wrapper: Using IRT.assistments data with problem_id for item_id_col, None for template_id_col, and None for concept_id_col
2018-02-26 10:20:08,741 [INFO] IRT.assistments: Read 525,534 rows from file
2018-02-26 10:20:08,741 [INFO] IRT.assistments: Dataframe index key is order_id
2018-02-26 10:20:09,015 [INFO] IRT.assistments: Removed 178,674 duplicate rows (346,860 rows remaining)
2018-02-26 10:20:09,500 [INFO] IRT.assistments: Removed students with <2 interactions (346,740 rows remaining)
2018-02-26 10:20:09,500 [INFO] IRT.assistments: maxInterNone mininter 2
2018-02-26 10:20:09,655 [INFO] IRT.assistments: concept_id_col not supplied, not using concepts
2018-02-26 10:20:09,655 [INFO] IRT.assistments: template_id_col not supplied, not using templates
2018-02-26 10:20:09,655 [INFO] IRT.assistments: Processed data: 346,740 interactions, 4,097 students; 26,684 items,   0 templates,   0 concepts columns to keep: ['user_idx', 'item_idx', 'correct', 'time_idx']
2018-02-26 10:20:09,696 [INFO] IRT.wrapper: After retaining proportional students, 346,740/346,740 rows and 4,097/4,097 students remain on 26,684 questions, 0.996828
2018-02-26 10:20:09,756 [INFO] IRT.run_mlp: this fold has 69348 interactions and students 3723
2018-02-26 10:20:09,756 [INFO] IRT.run_mlp: Beginning fold 1
2018-02-26 10:20:09,756 [INFO] IRT.run_mlp: Training RNN, fold 1, train length 277392, test length 69348
2018-02-26 10:20:09,756 [INFO] IRT.run_mlp: num_questions=26684? 26684 num_users=4097? 4097
/home/chen/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:337: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self.obj[key] = _infer_fill_value(value)
/home/chen/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:517: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy
  self.obj[item] = s
2018-02-26 10:20:09,848 [INFO] IRT.run_mlp: Building model: embedding size 200 hidden dimension 150
2018-02-26 10:20:10,178 [INFO] IRT.run_mlp: NCF input_dim=2*emb_size+1(timestamp) 401 hidden_num 150 output_dim 2
2018-02-26 10:20:10,179 [INFO] IRT.run_mlp: Model optimizer first learning rate is 0.001
2018-02-26 10:20:11,750 [INFO] IRT.run_mlp: Cuda detected, using cuda
2018-02-26 10:20:11,751 [INFO] IRT.run_mlp: Prediction output path data/Assistant/prediction/skill_builder_data.csv (removing existing one)!
2018-02-26 10:20:11,751 [INFO] IRT.run_mlp: Number of iterations for this fold 50
2018-02-26 10:20:11,751 [INFO] IRT.run_mlp: self.test_data_y.shape (69348, 2)
2018-02-26 10:20:21,217 [INFO] IRT.run_mlp: epoch 0 training loss (over all batches) 455.8429 
2018-02-26 10:20:31,699 [INFO] IRT.run_mlp: epoch 1 training loss (over all batches) 389.1861 
2018-02-26 10:20:45,405 [INFO] IRT.run_mlp: epoch 2 training loss (over all batches) 352.4838 
2018-02-26 10:20:59,132 [INFO] IRT.run_mlp: epoch 3 training loss (over all batches) 325.6407 
2018-02-26 10:21:12,495 [INFO] IRT.run_mlp: epoch 4 training loss (over all batches) 293.5536 
2018-02-26 10:21:26,178 [INFO] IRT.run_mlp: epoch 5 training loss (over all batches) 251.6526 
2018-02-26 10:21:39,842 [INFO] IRT.run_mlp: epoch 6 training loss (over all batches) 206.9192 
2018-02-26 10:21:53,541 [INFO] IRT.run_mlp: epoch 7 training loss (over all batches) 164.6344 
2018-02-26 10:22:07,231 [INFO] IRT.run_mlp: epoch 8 training loss (over all batches) 131.9879 
2018-02-26 10:22:20,982 [INFO] IRT.run_mlp: epoch 9 training loss (over all batches) 106.5802 
2018-02-26 10:22:34,723 [INFO] IRT.run_mlp: epoch 10 training loss (over all batches) 88.3815 
2018-02-26 10:22:34,745 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7124 auc 0.7470 
2018-02-26 10:22:48,473 [INFO] IRT.run_mlp: epoch 11 training loss (over all batches) 75.8945 
2018-02-26 10:23:02,216 [INFO] IRT.run_mlp: epoch 12 training loss (over all batches) 66.4823 
2018-02-26 10:23:15,901 [INFO] IRT.run_mlp: epoch 13 training loss (over all batches) 60.3066 
2018-02-26 10:23:29,624 [INFO] IRT.run_mlp: epoch 14 training loss (over all batches) 55.4142 
2018-02-26 10:23:43,360 [INFO] IRT.run_mlp: epoch 15 training loss (over all batches) 51.7299 
2018-02-26 10:23:57,114 [INFO] IRT.run_mlp: epoch 16 training loss (over all batches) 48.9314 
2018-02-26 10:24:10,877 [INFO] IRT.run_mlp: epoch 17 training loss (over all batches) 46.6246 
2018-02-26 10:24:24,625 [INFO] IRT.run_mlp: epoch 18 training loss (over all batches) 45.1151 
2018-02-26 10:24:38,461 [INFO] IRT.run_mlp: epoch 19 training loss (over all batches) 44.0874 
2018-02-26 10:24:52,147 [INFO] IRT.run_mlp: epoch 20 training loss (over all batches) 42.4648 
2018-02-26 10:24:52,168 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7152 auc 0.7494 
2018-02-26 10:25:05,888 [INFO] IRT.run_mlp: epoch 21 training loss (over all batches) 41.3395 
2018-02-26 10:25:19,574 [INFO] IRT.run_mlp: epoch 22 training loss (over all batches) 40.1103 
2018-02-26 10:25:33,292 [INFO] IRT.run_mlp: epoch 23 training loss (over all batches) 39.3638 
2018-02-26 10:25:47,108 [INFO] IRT.run_mlp: epoch 24 training loss (over all batches) 38.6638 
2018-02-26 10:26:00,886 [INFO] IRT.run_mlp: epoch 25 training loss (over all batches) 38.1784 
2018-02-26 10:26:14,583 [INFO] IRT.run_mlp: epoch 26 training loss (over all batches) 37.9027 
2018-02-26 10:26:28,257 [INFO] IRT.run_mlp: epoch 27 training loss (over all batches) 36.6506 
2018-02-26 10:26:41,990 [INFO] IRT.run_mlp: epoch 28 training loss (over all batches) 36.4541 
2018-02-26 10:26:55,741 [INFO] IRT.run_mlp: epoch 29 training loss (over all batches) 35.7911 
2018-02-26 10:27:09,552 [INFO] IRT.run_mlp: epoch 30 training loss (over all batches) 35.5812 
2018-02-26 10:27:09,573 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7187 auc 0.7520 
2018-02-26 10:27:23,329 [INFO] IRT.run_mlp: epoch 31 training loss (over all batches) 35.3276 
2018-02-26 10:27:37,060 [INFO] IRT.run_mlp: epoch 32 training loss (over all batches) 34.4247 
2018-02-26 10:27:50,841 [INFO] IRT.run_mlp: epoch 33 training loss (over all batches) 34.6616 
2018-02-26 10:28:04,538 [INFO] IRT.run_mlp: epoch 34 training loss (over all batches) 33.7524 
2018-02-26 10:28:18,203 [INFO] IRT.run_mlp: epoch 35 training loss (over all batches) 33.1782 
2018-02-26 10:28:31,933 [INFO] IRT.run_mlp: epoch 36 training loss (over all batches) 33.7809 
2018-02-26 10:28:45,625 [INFO] IRT.run_mlp: epoch 37 training loss (over all batches) 33.2801 
2018-02-26 10:28:59,376 [INFO] IRT.run_mlp: epoch 38 training loss (over all batches) 32.8973 
2018-02-26 10:29:13,108 [INFO] IRT.run_mlp: epoch 39 training loss (over all batches) 33.1950 
2018-02-26 10:29:26,819 [INFO] IRT.run_mlp: epoch 40 training loss (over all batches) 32.5005 
2018-02-26 10:29:26,839 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7224 auc 0.7506 
2018-02-26 10:29:40,562 [INFO] IRT.run_mlp: epoch 41 training loss (over all batches) 32.8973 
2018-02-26 10:29:54,337 [INFO] IRT.run_mlp: epoch 42 training loss (over all batches) 32.1213 
2018-02-26 10:30:08,063 [INFO] IRT.run_mlp: epoch 43 training loss (over all batches) 32.3321 
2018-02-26 10:30:21,794 [INFO] IRT.run_mlp: epoch 44 training loss (over all batches) 31.6395 
2018-02-26 10:30:35,485 [INFO] IRT.run_mlp: epoch 45 training loss (over all batches) 31.8743 
2018-02-26 10:30:49,243 [INFO] IRT.run_mlp: epoch 46 training loss (over all batches) 32.0463 
2018-02-26 10:31:03,038 [INFO] IRT.run_mlp: epoch 47 training loss (over all batches) 31.4087 
2018-02-26 10:31:16,791 [INFO] IRT.run_mlp: epoch 48 training loss (over all batches) 31.2308 
2018-02-26 10:31:30,545 [INFO] IRT.run_mlp: epoch 49 training loss (over all batches) 31.3986 
2018-02-26 10:31:30,552 [INFO] IRT.run_mlp: Appending this fold prediction to data/Assistant/prediction/skill_builder_data.csv, number of interactions 69348
2018-02-26 10:31:30,773 [INFO] IRT.run_mlp: Fold 1: Num Interactions: 69348; Test Accuracy: 0.72239; Test AUC: 0.75061
2018-02-26 10:31:30,817 [INFO] IRT.run_mlp: this fold has 69348 interactions and students 3716
2018-02-26 10:31:30,817 [INFO] IRT.run_mlp: Beginning fold 2
2018-02-26 10:31:30,817 [INFO] IRT.run_mlp: Training RNN, fold 2, train length 277392, test length 69348
2018-02-26 10:31:30,817 [INFO] IRT.run_mlp: num_questions=26684? 26684 num_users=4097? 4097
2018-02-26 10:31:30,905 [INFO] IRT.run_mlp: Building model: embedding size 200 hidden dimension 150
2018-02-26 10:31:31,233 [INFO] IRT.run_mlp: NCF input_dim=2*emb_size+1(timestamp) 401 hidden_num 150 output_dim 2
2018-02-26 10:31:31,233 [INFO] IRT.run_mlp: Model optimizer first learning rate is 0.001
2018-02-26 10:31:31,242 [INFO] IRT.run_mlp: Cuda detected, using cuda
2018-02-26 10:31:31,243 [INFO] IRT.run_mlp: Prediction output path data/Assistant/prediction/skill_builder_data.csv (removing existing one)!
2018-02-26 10:31:31,243 [INFO] IRT.run_mlp: Number of iterations for this fold 50
2018-02-26 10:31:31,243 [INFO] IRT.run_mlp: self.test_data_y.shape (69348, 2)
2018-02-26 10:31:44,967 [INFO] IRT.run_mlp: epoch 0 training loss (over all batches) 455.0556 
2018-02-26 10:31:58,451 [INFO] IRT.run_mlp: epoch 1 training loss (over all batches) 388.4633 
2018-02-26 10:32:12,195 [INFO] IRT.run_mlp: epoch 2 training loss (over all batches) 352.5544 
2018-02-26 10:32:25,894 [INFO] IRT.run_mlp: epoch 3 training loss (over all batches) 326.2910 
2018-02-26 10:32:39,312 [INFO] IRT.run_mlp: epoch 4 training loss (over all batches) 294.4153 
2018-02-26 10:32:53,030 [INFO] IRT.run_mlp: epoch 5 training loss (over all batches) 253.4138 
2018-02-26 10:33:06,741 [INFO] IRT.run_mlp: epoch 6 training loss (over all batches) 208.3464 
2018-02-26 10:33:20,492 [INFO] IRT.run_mlp: epoch 7 training loss (over all batches) 166.2287 
2018-02-26 10:33:34,214 [INFO] IRT.run_mlp: epoch 8 training loss (over all batches) 131.7646 
2018-02-26 10:33:47,877 [INFO] IRT.run_mlp: epoch 9 training loss (over all batches) 107.0646 
2018-02-26 10:34:01,603 [INFO] IRT.run_mlp: epoch 10 training loss (over all batches) 87.8948 
2018-02-26 10:34:01,626 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7058 auc 0.7433 
2018-02-26 10:34:15,390 [INFO] IRT.run_mlp: epoch 11 training loss (over all batches) 75.3851 
2018-02-26 10:34:29,106 [INFO] IRT.run_mlp: epoch 12 training loss (over all batches) 66.5520 
2018-02-26 10:34:42,877 [INFO] IRT.run_mlp: epoch 13 training loss (over all batches) 59.2173 
2018-02-26 10:34:56,674 [INFO] IRT.run_mlp: epoch 14 training loss (over all batches) 55.3192 
2018-02-26 10:35:10,360 [INFO] IRT.run_mlp: epoch 15 training loss (over all batches) 51.6520 
2018-02-26 10:35:24,101 [INFO] IRT.run_mlp: epoch 16 training loss (over all batches) 48.7811 
2018-02-26 10:35:37,843 [INFO] IRT.run_mlp: epoch 17 training loss (over all batches) 46.7622 
2018-02-26 10:35:51,655 [INFO] IRT.run_mlp: epoch 18 training loss (over all batches) 44.5239 
2018-02-26 10:36:05,416 [INFO] IRT.run_mlp: epoch 19 training loss (over all batches) 43.2015 
2018-02-26 10:36:19,101 [INFO] IRT.run_mlp: epoch 20 training loss (over all batches) 41.8785 
2018-02-26 10:36:19,122 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7125 auc 0.7494 
2018-02-26 10:36:32,820 [INFO] IRT.run_mlp: epoch 21 training loss (over all batches) 40.8619 
2018-02-26 10:36:46,568 [INFO] IRT.run_mlp: epoch 22 training loss (over all batches) 39.9195 
2018-02-26 10:37:00,388 [INFO] IRT.run_mlp: epoch 23 training loss (over all batches) 38.8107 
2018-02-26 10:37:14,166 [INFO] IRT.run_mlp: epoch 24 training loss (over all batches) 38.4977 
2018-02-26 10:37:27,941 [INFO] IRT.run_mlp: epoch 25 training loss (over all batches) 37.0507 
2018-02-26 10:37:41,672 [INFO] IRT.run_mlp: epoch 26 training loss (over all batches) 36.9689 
2018-02-26 10:37:55,375 [INFO] IRT.run_mlp: epoch 27 training loss (over all batches) 36.1930 
2018-02-26 10:38:09,141 [INFO] IRT.run_mlp: epoch 28 training loss (over all batches) 36.2874 
2018-02-26 10:38:22,819 [INFO] IRT.run_mlp: epoch 29 training loss (over all batches) 35.3776 
2018-02-26 10:38:36,577 [INFO] IRT.run_mlp: epoch 30 training loss (over all batches) 34.9662 
2018-02-26 10:38:36,598 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7201 auc 0.7520 
2018-02-26 10:38:50,329 [INFO] IRT.run_mlp: epoch 31 training loss (over all batches) 34.3655 
2018-02-26 10:39:04,069 [INFO] IRT.run_mlp: epoch 32 training loss (over all batches) 34.8267 
2018-02-26 10:39:17,800 [INFO] IRT.run_mlp: epoch 33 training loss (over all batches) 34.1155 
2018-02-26 10:39:31,492 [INFO] IRT.run_mlp: epoch 34 training loss (over all batches) 33.6270 
2018-02-26 10:39:45,196 [INFO] IRT.run_mlp: epoch 35 training loss (over all batches) 33.6992 
2018-02-26 10:39:58,906 [INFO] IRT.run_mlp: epoch 36 training loss (over all batches) 32.8683 
2018-02-26 10:40:12,667 [INFO] IRT.run_mlp: epoch 37 training loss (over all batches) 33.3846 
2018-02-26 10:40:26,362 [INFO] IRT.run_mlp: epoch 38 training loss (over all batches) 33.1542 
2018-02-26 10:40:40,145 [INFO] IRT.run_mlp: epoch 39 training loss (over all batches) 32.3783 
2018-02-26 10:40:53,878 [INFO] IRT.run_mlp: epoch 40 training loss (over all batches) 32.2151 
2018-02-26 10:40:53,899 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7235 auc 0.7533 
2018-02-26 10:41:07,601 [INFO] IRT.run_mlp: epoch 41 training loss (over all batches) 32.0862 
2018-02-26 10:41:21,309 [INFO] IRT.run_mlp: epoch 42 training loss (over all batches) 31.9814 
2018-02-26 10:41:35,027 [INFO] IRT.run_mlp: epoch 43 training loss (over all batches) 32.0896 
2018-02-26 10:41:48,757 [INFO] IRT.run_mlp: epoch 44 training loss (over all batches) 31.9463 
2018-02-26 10:42:02,524 [INFO] IRT.run_mlp: epoch 45 training loss (over all batches) 31.5360 
2018-02-26 10:42:16,343 [INFO] IRT.run_mlp: epoch 46 training loss (over all batches) 31.5722 
2018-02-26 10:42:30,074 [INFO] IRT.run_mlp: epoch 47 training loss (over all batches) 31.1300 
2018-02-26 10:42:43,875 [INFO] IRT.run_mlp: epoch 48 training loss (over all batches) 31.3920 
2018-02-26 10:42:57,584 [INFO] IRT.run_mlp: epoch 49 training loss (over all batches) 30.9262 
2018-02-26 10:42:57,590 [INFO] IRT.run_mlp: Appending this fold prediction to data/Assistant/prediction/skill_builder_data.csv, number of interactions 69348
2018-02-26 10:42:57,810 [INFO] IRT.run_mlp: Fold 2: Num Interactions: 69348; Test Accuracy: 0.72354; Test AUC: 0.75328
2018-02-26 10:42:57,852 [INFO] IRT.run_mlp: this fold has 69348 interactions and students 3730
2018-02-26 10:42:57,853 [INFO] IRT.run_mlp: Beginning fold 3
2018-02-26 10:42:57,853 [INFO] IRT.run_mlp: Training RNN, fold 3, train length 277392, test length 69348
2018-02-26 10:42:57,853 [INFO] IRT.run_mlp: num_questions=26684? 26684 num_users=4097? 4097
2018-02-26 10:42:57,941 [INFO] IRT.run_mlp: Building model: embedding size 200 hidden dimension 150
2018-02-26 10:42:58,269 [INFO] IRT.run_mlp: NCF input_dim=2*emb_size+1(timestamp) 401 hidden_num 150 output_dim 2
2018-02-26 10:42:58,269 [INFO] IRT.run_mlp: Model optimizer first learning rate is 0.001
2018-02-26 10:42:58,277 [INFO] IRT.run_mlp: Cuda detected, using cuda
2018-02-26 10:42:58,277 [INFO] IRT.run_mlp: Prediction output path data/Assistant/prediction/skill_builder_data.csv (removing existing one)!
2018-02-26 10:42:58,278 [INFO] IRT.run_mlp: Number of iterations for this fold 50
2018-02-26 10:42:58,278 [INFO] IRT.run_mlp: self.test_data_y.shape (69348, 2)
2018-02-26 10:43:11,971 [INFO] IRT.run_mlp: epoch 0 training loss (over all batches) 457.0149 
2018-02-26 10:43:25,383 [INFO] IRT.run_mlp: epoch 1 training loss (over all batches) 389.4543 
2018-02-26 10:43:39,116 [INFO] IRT.run_mlp: epoch 2 training loss (over all batches) 351.9300 
2018-02-26 10:43:52,858 [INFO] IRT.run_mlp: epoch 3 training loss (over all batches) 323.7913 
2018-02-26 10:44:06,309 [INFO] IRT.run_mlp: epoch 4 training loss (over all batches) 289.8403 
2018-02-26 10:44:19,967 [INFO] IRT.run_mlp: epoch 5 training loss (over all batches) 247.2281 
2018-02-26 10:44:33,677 [INFO] IRT.run_mlp: epoch 6 training loss (over all batches) 201.3468 
2018-02-26 10:44:47,453 [INFO] IRT.run_mlp: epoch 7 training loss (over all batches) 160.2990 
2018-02-26 10:45:01,125 [INFO] IRT.run_mlp: epoch 8 training loss (over all batches) 126.9323 
2018-02-26 10:45:14,828 [INFO] IRT.run_mlp: epoch 9 training loss (over all batches) 103.3361 
2018-02-26 10:45:28,560 [INFO] IRT.run_mlp: epoch 10 training loss (over all batches) 85.2302 
2018-02-26 10:45:28,583 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7116 auc 0.7479 
2018-02-26 10:45:42,355 [INFO] IRT.run_mlp: epoch 11 training loss (over all batches) 72.7735 
2018-02-26 10:45:56,064 [INFO] IRT.run_mlp: epoch 12 training loss (over all batches) 65.1421 
2018-02-26 10:46:09,790 [INFO] IRT.run_mlp: epoch 13 training loss (over all batches) 59.0554 
2018-02-26 10:46:23,550 [INFO] IRT.run_mlp: epoch 14 training loss (over all batches) 54.5161 
2018-02-26 10:46:37,198 [INFO] IRT.run_mlp: epoch 15 training loss (over all batches) 51.0112 
2018-02-26 10:46:51,003 [INFO] IRT.run_mlp: epoch 16 training loss (over all batches) 48.4912 
2018-02-26 10:47:04,757 [INFO] IRT.run_mlp: epoch 17 training loss (over all batches) 46.2858 
2018-02-26 10:47:18,497 [INFO] IRT.run_mlp: epoch 18 training loss (over all batches) 44.4487 
2018-02-26 10:47:32,244 [INFO] IRT.run_mlp: epoch 19 training loss (over all batches) 43.0538 
2018-02-26 10:47:45,974 [INFO] IRT.run_mlp: epoch 20 training loss (over all batches) 42.2520 
2018-02-26 10:47:45,995 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7156 auc 0.7520 
2018-02-26 10:47:59,737 [INFO] IRT.run_mlp: epoch 21 training loss (over all batches) 40.3478 
2018-02-26 10:48:13,468 [INFO] IRT.run_mlp: epoch 22 training loss (over all batches) 40.2857 
2018-02-26 10:48:27,258 [INFO] IRT.run_mlp: epoch 23 training loss (over all batches) 38.9245 
2018-02-26 10:48:41,035 [INFO] IRT.run_mlp: epoch 24 training loss (over all batches) 38.3402 
2018-02-26 10:48:54,744 [INFO] IRT.run_mlp: epoch 25 training loss (over all batches) 37.3934 
2018-02-26 10:49:08,462 [INFO] IRT.run_mlp: epoch 26 training loss (over all batches) 36.7969 
2018-02-26 10:49:22,242 [INFO] IRT.run_mlp: epoch 27 training loss (over all batches) 36.5421 
2018-02-26 10:49:35,933 [INFO] IRT.run_mlp: epoch 28 training loss (over all batches) 36.4602 
2018-02-26 10:49:49,607 [INFO] IRT.run_mlp: epoch 29 training loss (over all batches) 35.8075 
2018-02-26 10:50:03,363 [INFO] IRT.run_mlp: epoch 30 training loss (over all batches) 35.3472 
2018-02-26 10:50:03,385 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7264 auc 0.7541 
2018-02-26 10:50:17,057 [INFO] IRT.run_mlp: epoch 31 training loss (over all batches) 34.9203 
2018-02-26 10:50:30,810 [INFO] IRT.run_mlp: epoch 32 training loss (over all batches) 34.9128 
2018-02-26 10:50:44,556 [INFO] IRT.run_mlp: epoch 33 training loss (over all batches) 34.3862 
2018-02-26 10:50:58,363 [INFO] IRT.run_mlp: epoch 34 training loss (over all batches) 33.6527 
2018-02-26 10:51:12,071 [INFO] IRT.run_mlp: epoch 35 training loss (over all batches) 33.9876 
2018-02-26 10:51:25,797 [INFO] IRT.run_mlp: epoch 36 training loss (over all batches) 33.3978 
2018-02-26 10:51:39,511 [INFO] IRT.run_mlp: epoch 37 training loss (over all batches) 33.3072 
2018-02-26 10:51:53,194 [INFO] IRT.run_mlp: epoch 38 training loss (over all batches) 32.9175 
2018-02-26 10:52:06,961 [INFO] IRT.run_mlp: epoch 39 training loss (over all batches) 32.8995 
2018-02-26 10:52:20,682 [INFO] IRT.run_mlp: epoch 40 training loss (over all batches) 32.2632 
2018-02-26 10:52:20,703 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7222 auc 0.7513 
2018-02-26 10:52:34,468 [INFO] IRT.run_mlp: epoch 41 training loss (over all batches) 32.8329 
2018-02-26 10:52:48,235 [INFO] IRT.run_mlp: epoch 42 training loss (over all batches) 32.0693 
2018-02-26 10:53:01,918 [INFO] IRT.run_mlp: epoch 43 training loss (over all batches) 32.1559 
2018-02-26 10:53:15,609 [INFO] IRT.run_mlp: epoch 44 training loss (over all batches) 31.9242 
2018-02-26 10:53:29,280 [INFO] IRT.run_mlp: epoch 45 training loss (over all batches) 31.7893 
2018-02-26 10:53:42,980 [INFO] IRT.run_mlp: epoch 46 training loss (over all batches) 31.7786 
2018-02-26 10:53:56,741 [INFO] IRT.run_mlp: epoch 47 training loss (over all batches) 31.7249 
2018-02-26 10:54:10,450 [INFO] IRT.run_mlp: epoch 48 training loss (over all batches) 31.7090 
2018-02-26 10:54:24,205 [INFO] IRT.run_mlp: epoch 49 training loss (over all batches) 30.9496 
2018-02-26 10:54:24,213 [INFO] IRT.run_mlp: Appending this fold prediction to data/Assistant/prediction/skill_builder_data.csv, number of interactions 69348
2018-02-26 10:54:24,433 [INFO] IRT.run_mlp: Fold 3: Num Interactions: 69348; Test Accuracy: 0.72218; Test AUC: 0.75134
2018-02-26 10:54:24,477 [INFO] IRT.run_mlp: this fold has 69348 interactions and students 3732
2018-02-26 10:54:24,477 [INFO] IRT.run_mlp: Beginning fold 4
2018-02-26 10:54:24,477 [INFO] IRT.run_mlp: Training RNN, fold 4, train length 277392, test length 69348
2018-02-26 10:54:24,477 [INFO] IRT.run_mlp: num_questions=26684? 26684 num_users=4097? 4097
2018-02-26 10:54:24,564 [INFO] IRT.run_mlp: Building model: embedding size 200 hidden dimension 150
2018-02-26 10:54:24,890 [INFO] IRT.run_mlp: NCF input_dim=2*emb_size+1(timestamp) 401 hidden_num 150 output_dim 2
2018-02-26 10:54:24,890 [INFO] IRT.run_mlp: Model optimizer first learning rate is 0.001
2018-02-26 10:54:24,900 [INFO] IRT.run_mlp: Cuda detected, using cuda
2018-02-26 10:54:24,900 [INFO] IRT.run_mlp: Prediction output path data/Assistant/prediction/skill_builder_data.csv (removing existing one)!
2018-02-26 10:54:24,900 [INFO] IRT.run_mlp: Number of iterations for this fold 50
2018-02-26 10:54:24,900 [INFO] IRT.run_mlp: self.test_data_y.shape (69348, 2)
2018-02-26 10:54:38,622 [INFO] IRT.run_mlp: epoch 0 training loss (over all batches) 456.8335 
2018-02-26 10:54:52,016 [INFO] IRT.run_mlp: epoch 1 training loss (over all batches) 389.2886 
2018-02-26 10:55:05,659 [INFO] IRT.run_mlp: epoch 2 training loss (over all batches) 351.7340 
2018-02-26 10:55:19,426 [INFO] IRT.run_mlp: epoch 3 training loss (over all batches) 324.1176 
2018-02-26 10:55:32,862 [INFO] IRT.run_mlp: epoch 4 training loss (over all batches) 290.1947 
2018-02-26 10:55:46,580 [INFO] IRT.run_mlp: epoch 5 training loss (over all batches) 247.4915 
2018-02-26 10:56:00,207 [INFO] IRT.run_mlp: epoch 6 training loss (over all batches) 201.3182 
2018-02-26 10:56:13,978 [INFO] IRT.run_mlp: epoch 7 training loss (over all batches) 159.7365 
2018-02-26 10:56:27,659 [INFO] IRT.run_mlp: epoch 8 training loss (over all batches) 127.0656 
2018-02-26 10:56:41,425 [INFO] IRT.run_mlp: epoch 9 training loss (over all batches) 101.6581 
2018-02-26 10:56:55,217 [INFO] IRT.run_mlp: epoch 10 training loss (over all batches) 85.1107 
2018-02-26 10:56:55,241 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7030 auc 0.7422 
2018-02-26 10:57:08,896 [INFO] IRT.run_mlp: epoch 11 training loss (over all batches) 73.2863 
2018-02-26 10:57:22,559 [INFO] IRT.run_mlp: epoch 12 training loss (over all batches) 64.8438 
2018-02-26 10:57:36,283 [INFO] IRT.run_mlp: epoch 13 training loss (over all batches) 58.5741 
2018-02-26 10:57:50,064 [INFO] IRT.run_mlp: epoch 14 training loss (over all batches) 54.5498 
2018-02-26 10:58:03,707 [INFO] IRT.run_mlp: epoch 15 training loss (over all batches) 50.7738 
2018-02-26 10:58:17,456 [INFO] IRT.run_mlp: epoch 16 training loss (over all batches) 48.7739 
2018-02-26 10:58:31,138 [INFO] IRT.run_mlp: epoch 17 training loss (over all batches) 46.3387 
2018-02-26 10:58:44,865 [INFO] IRT.run_mlp: epoch 18 training loss (over all batches) 44.1536 
2018-02-26 10:58:58,547 [INFO] IRT.run_mlp: epoch 19 training loss (over all batches) 42.8414 
2018-02-26 10:59:12,292 [INFO] IRT.run_mlp: epoch 20 training loss (over all batches) 41.4834 
2018-02-26 10:59:12,313 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7104 auc 0.7472 
2018-02-26 10:59:26,084 [INFO] IRT.run_mlp: epoch 21 training loss (over all batches) 40.8699 
2018-02-26 10:59:39,784 [INFO] IRT.run_mlp: epoch 22 training loss (over all batches) 39.6239 
2018-02-26 10:59:53,465 [INFO] IRT.run_mlp: epoch 23 training loss (over all batches) 38.3422 
2018-02-26 11:00:07,140 [INFO] IRT.run_mlp: epoch 24 training loss (over all batches) 38.2385 
2018-02-26 11:00:20,901 [INFO] IRT.run_mlp: epoch 25 training loss (over all batches) 37.2296 
2018-02-26 11:00:34,679 [INFO] IRT.run_mlp: epoch 26 training loss (over all batches) 36.9774 
2018-02-26 11:00:48,404 [INFO] IRT.run_mlp: epoch 27 training loss (over all batches) 36.4137 
2018-02-26 11:01:02,132 [INFO] IRT.run_mlp: epoch 28 training loss (over all batches) 35.6731 
2018-02-26 11:01:15,895 [INFO] IRT.run_mlp: epoch 29 training loss (over all batches) 35.1646 
2018-02-26 11:01:29,623 [INFO] IRT.run_mlp: epoch 30 training loss (over all batches) 35.1688 
2018-02-26 11:01:29,644 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7190 auc 0.7478 
2018-02-26 11:01:43,365 [INFO] IRT.run_mlp: epoch 31 training loss (over all batches) 34.4814 
2018-02-26 11:01:57,047 [INFO] IRT.run_mlp: epoch 32 training loss (over all batches) 34.3332 
2018-02-26 11:02:10,771 [INFO] IRT.run_mlp: epoch 33 training loss (over all batches) 34.4400 
2018-02-26 11:02:24,526 [INFO] IRT.run_mlp: epoch 34 training loss (over all batches) 33.6867 
2018-02-26 11:02:38,228 [INFO] IRT.run_mlp: epoch 35 training loss (over all batches) 33.6172 
2018-02-26 11:02:51,967 [INFO] IRT.run_mlp: epoch 36 training loss (over all batches) 32.9676 
2018-02-26 11:03:05,672 [INFO] IRT.run_mlp: epoch 37 training loss (over all batches) 33.0631 
2018-02-26 11:03:19,408 [INFO] IRT.run_mlp: epoch 38 training loss (over all batches) 32.7532 
2018-02-26 11:03:33,108 [INFO] IRT.run_mlp: epoch 39 training loss (over all batches) 32.7294 
2018-02-26 11:03:46,812 [INFO] IRT.run_mlp: epoch 40 training loss (over all batches) 32.5405 
2018-02-26 11:03:46,833 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7179 auc 0.7477 
2018-02-26 11:04:00,602 [INFO] IRT.run_mlp: epoch 41 training loss (over all batches) 32.4058 
2018-02-26 11:04:14,313 [INFO] IRT.run_mlp: epoch 42 training loss (over all batches) 32.0419 
2018-02-26 11:04:28,033 [INFO] IRT.run_mlp: epoch 43 training loss (over all batches) 31.6457 
2018-02-26 11:04:41,673 [INFO] IRT.run_mlp: epoch 44 training loss (over all batches) 31.8776 
2018-02-26 11:04:55,426 [INFO] IRT.run_mlp: epoch 45 training loss (over all batches) 31.6162 
2018-02-26 11:05:09,184 [INFO] IRT.run_mlp: epoch 46 training loss (over all batches) 31.3980 
2018-02-26 11:05:22,958 [INFO] IRT.run_mlp: epoch 47 training loss (over all batches) 31.5326 
2018-02-26 11:05:36,654 [INFO] IRT.run_mlp: epoch 48 training loss (over all batches) 30.7351 
2018-02-26 11:05:50,329 [INFO] IRT.run_mlp: epoch 49 training loss (over all batches) 30.7210 
2018-02-26 11:05:50,337 [INFO] IRT.run_mlp: Appending this fold prediction to data/Assistant/prediction/skill_builder_data.csv, number of interactions 69348
2018-02-26 11:05:50,557 [INFO] IRT.run_mlp: Fold 4: Num Interactions: 69348; Test Accuracy: 0.71790; Test AUC: 0.74766
2018-02-26 11:05:50,600 [INFO] IRT.run_mlp: this fold has 69348 interactions and students 3731
2018-02-26 11:05:50,600 [INFO] IRT.run_mlp: Beginning fold 5
2018-02-26 11:05:50,600 [INFO] IRT.run_mlp: Training RNN, fold 5, train length 277392, test length 69348
2018-02-26 11:05:50,600 [INFO] IRT.run_mlp: num_questions=26684? 26684 num_users=4097? 4097
2018-02-26 11:05:50,686 [INFO] IRT.run_mlp: Building model: embedding size 200 hidden dimension 150
2018-02-26 11:05:51,013 [INFO] IRT.run_mlp: NCF input_dim=2*emb_size+1(timestamp) 401 hidden_num 150 output_dim 2
2018-02-26 11:05:51,013 [INFO] IRT.run_mlp: Model optimizer first learning rate is 0.001
2018-02-26 11:05:51,022 [INFO] IRT.run_mlp: Cuda detected, using cuda
2018-02-26 11:05:51,022 [INFO] IRT.run_mlp: Prediction output path data/Assistant/prediction/skill_builder_data.csv (removing existing one)!
2018-02-26 11:05:51,022 [INFO] IRT.run_mlp: Number of iterations for this fold 50
2018-02-26 11:05:51,022 [INFO] IRT.run_mlp: self.test_data_y.shape (69348, 2)
2018-02-26 11:06:04,726 [INFO] IRT.run_mlp: epoch 0 training loss (over all batches) 455.7424 
2018-02-26 11:06:18,117 [INFO] IRT.run_mlp: epoch 1 training loss (over all batches) 388.6473 
2018-02-26 11:06:31,874 [INFO] IRT.run_mlp: epoch 2 training loss (over all batches) 351.5069 
2018-02-26 11:06:45,593 [INFO] IRT.run_mlp: epoch 3 training loss (over all batches) 324.1772 
2018-02-26 11:06:59,306 [INFO] IRT.run_mlp: epoch 4 training loss (over all batches) 290.9864 
2018-02-26 11:07:09,026 [INFO] IRT.run_mlp: epoch 5 training loss (over all batches) 249.3778 
2018-02-26 11:07:18,374 [INFO] IRT.run_mlp: epoch 6 training loss (over all batches) 204.7023 
2018-02-26 11:07:27,694 [INFO] IRT.run_mlp: epoch 7 training loss (over all batches) 163.5239 
2018-02-26 11:07:37,088 [INFO] IRT.run_mlp: epoch 8 training loss (over all batches) 130.7980 
2018-02-26 11:07:46,498 [INFO] IRT.run_mlp: epoch 9 training loss (over all batches) 105.7918 
2018-02-26 11:07:55,860 [INFO] IRT.run_mlp: epoch 10 training loss (over all batches) 87.6980 
2018-02-26 11:07:55,880 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7071 auc 0.7432 
2018-02-26 11:08:05,240 [INFO] IRT.run_mlp: epoch 11 training loss (over all batches) 75.2111 
2018-02-26 11:08:14,623 [INFO] IRT.run_mlp: epoch 12 training loss (over all batches) 66.2385 
2018-02-26 11:08:24,005 [INFO] IRT.run_mlp: epoch 13 training loss (over all batches) 60.3599 
2018-02-26 11:08:33,353 [INFO] IRT.run_mlp: epoch 14 training loss (over all batches) 55.6528 
2018-02-26 11:08:42,695 [INFO] IRT.run_mlp: epoch 15 training loss (over all batches) 51.7414 
2018-02-26 11:08:52,075 [INFO] IRT.run_mlp: epoch 16 training loss (over all batches) 49.1838 
2018-02-26 11:09:01,460 [INFO] IRT.run_mlp: epoch 17 training loss (over all batches) 46.9911 
2018-02-26 11:09:10,818 [INFO] IRT.run_mlp: epoch 18 training loss (over all batches) 45.3855 
2018-02-26 11:09:20,204 [INFO] IRT.run_mlp: epoch 19 training loss (over all batches) 44.0693 
2018-02-26 11:09:29,546 [INFO] IRT.run_mlp: epoch 20 training loss (over all batches) 42.4643 
2018-02-26 11:09:29,565 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7125 auc 0.7477 
2018-02-26 11:09:38,939 [INFO] IRT.run_mlp: epoch 21 training loss (over all batches) 41.6007 
2018-02-26 11:09:48,318 [INFO] IRT.run_mlp: epoch 22 training loss (over all batches) 40.6571 
2018-02-26 11:09:57,708 [INFO] IRT.run_mlp: epoch 23 training loss (over all batches) 39.5459 
2018-02-26 11:10:07,103 [INFO] IRT.run_mlp: epoch 24 training loss (over all batches) 38.9736 
2018-02-26 11:10:16,475 [INFO] IRT.run_mlp: epoch 25 training loss (over all batches) 38.5586 
2018-02-26 11:10:25,874 [INFO] IRT.run_mlp: epoch 26 training loss (over all batches) 38.0979 
2018-02-26 11:10:35,281 [INFO] IRT.run_mlp: epoch 27 training loss (over all batches) 37.5788 
2018-02-26 11:10:44,619 [INFO] IRT.run_mlp: epoch 28 training loss (over all batches) 37.0787 
2018-02-26 11:10:53,991 [INFO] IRT.run_mlp: epoch 29 training loss (over all batches) 35.9687 
2018-02-26 11:11:03,367 [INFO] IRT.run_mlp: epoch 30 training loss (over all batches) 36.3033 
2018-02-26 11:11:03,386 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7159 auc 0.7480 
2018-02-26 11:11:12,723 [INFO] IRT.run_mlp: epoch 31 training loss (over all batches) 35.7670 
2018-02-26 11:11:22,095 [INFO] IRT.run_mlp: epoch 32 training loss (over all batches) 35.7950 
2018-02-26 11:11:31,470 [INFO] IRT.run_mlp: epoch 33 training loss (over all batches) 35.1944 
2018-02-26 11:11:40,885 [INFO] IRT.run_mlp: epoch 34 training loss (over all batches) 34.5097 
2018-02-26 11:11:50,306 [INFO] IRT.run_mlp: epoch 35 training loss (over all batches) 34.8479 
2018-02-26 11:11:59,677 [INFO] IRT.run_mlp: epoch 36 training loss (over all batches) 34.4057 
2018-02-26 11:12:09,021 [INFO] IRT.run_mlp: epoch 37 training loss (over all batches) 33.8202 
2018-02-26 11:12:18,396 [INFO] IRT.run_mlp: epoch 38 training loss (over all batches) 34.0294 
2018-02-26 11:12:27,778 [INFO] IRT.run_mlp: epoch 39 training loss (over all batches) 33.8740 
2018-02-26 11:12:37,150 [INFO] IRT.run_mlp: epoch 40 training loss (over all batches) 33.4437 
2018-02-26 11:12:37,169 [INFO] IRT.run_mlp: testing every 10 accuracy 0.7208 auc 0.7489 
2018-02-26 11:12:46,529 [INFO] IRT.run_mlp: epoch 41 training loss (over all batches) 32.9847 
2018-02-26 11:12:55,903 [INFO] IRT.run_mlp: epoch 42 training loss (over all batches) 33.0460 
2018-02-26 11:13:05,283 [INFO] IRT.run_mlp: epoch 43 training loss (over all batches) 32.7634 
2018-02-26 11:13:14,637 [INFO] IRT.run_mlp: epoch 44 training loss (over all batches) 32.8391 
2018-02-26 11:13:23,991 [INFO] IRT.run_mlp: epoch 45 training loss (over all batches) 32.2071 
2018-02-26 11:13:33,368 [INFO] IRT.run_mlp: epoch 46 training loss (over all batches) 32.2068 
2018-02-26 11:13:42,700 [INFO] IRT.run_mlp: epoch 47 training loss (over all batches) 31.9979 
2018-02-26 11:13:52,112 [INFO] IRT.run_mlp: epoch 48 training loss (over all batches) 32.3215 
2018-02-26 11:14:01,491 [INFO] IRT.run_mlp: epoch 49 training loss (over all batches) 31.9046 
2018-02-26 11:14:01,497 [INFO] IRT.run_mlp: Appending this fold prediction to data/Assistant/prediction/skill_builder_data.csv, number of interactions 69348
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Fold 5: Num Interactions: 69348; Test Accuracy: 0.72079; Test AUC: 0.74887
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Completed all 5 folds
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Fold 1 Acc: 0.72239 AUC: 0.75061
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Fold 2 Acc: 0.72354 AUC: 0.75328
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Fold 3 Acc: 0.72218 AUC: 0.75134
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Fold 4 Acc: 0.71790 AUC: 0.74766
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Fold 5 Acc: 0.72079 AUC: 0.74887
2018-02-26 11:14:01,714 [INFO] IRT.run_mlp: Overall 5 Acc: 0.72136 AUC: 0.75035
